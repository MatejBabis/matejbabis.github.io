<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Blogs on Matej Babis</title>
    <link>http://localhost:1313/blog/</link>
    <description>Recent content in Blogs on Matej Babis</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <copyright>Copyright © 2024, Matej Babis.</copyright>
    <lastBuildDate>Wed, 26 Mar 2025 11:18:39 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/blog/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>New GPT-4o: Image Generation for Everyone</title>
      <link>http://localhost:1313/new-gpt-4o-image-generation-for-everyone/</link>
      <pubDate>Wed, 26 Mar 2025 11:18:39 +0100</pubDate>
      <guid>http://localhost:1313/new-gpt-4o-image-generation-for-everyone/</guid>
      <description>&lt;p&gt;New &lt;a href=&#34;https://openai.com/index/introducing-4o-image-generation/&#34;&gt;updates&lt;/a&gt; to OpenAI&amp;rsquo;s 4o model introduce the ability to output images, extending the existing capability to take images as inputs. This brings OpenAI back to the top players of the image generation field as their previous model DALL-E has now been lacking behind &lt;a href=&#34;https://x.com/ArtificialAnlys/status/1904188980423467472&#34;&gt;top players&lt;/a&gt; in the field that include both image generation-focused start ups like &lt;a href=&#34;https://preview.reve.art/&#34;&gt;Reve&lt;/a&gt; or corporate rivals such as Google &lt;a href=&#34;https://deepmind.google/technologies/imagen-3/&#34;&gt;Imagen 3&lt;/a&gt;.&lt;/p&gt;&#xA;&lt;p&gt;The ability to output images is something I have seen requested many times by customers defining LLM-based projects. Especially among the general public it was often a surprise that models this powerful, able to take audio or image inputs cannot return them altered. OpenAI, with its massive ChatGPT user base now addresses this issue, announcing it will provide the new model verisons to all customers, including the &lt;a href=&#34;https://openai.com/index/introducing-4o-image-generation/#access-and-availability&#34;&gt;free tier&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>DeepSeek-R1 &amp; Chinese Censorship</title>
      <link>http://localhost:1313/deepseek-r1-chinese-censorship/</link>
      <pubDate>Sat, 25 Jan 2025 11:30:52 +0100</pubDate>
      <guid>http://localhost:1313/deepseek-r1-chinese-censorship/</guid>
      <description>&lt;p&gt;DeepSeek-R1 is &lt;a href=&#34;https://www.forbes.com/sites/craigsmith/2025/01/22/deepseek-how-chinas-ai-innovators-are-challenging-the-status-quo/&#34;&gt;taking the world&lt;/a&gt; &lt;a href=&#34;https://www.nytimes.com/2025/01/23/technology/deepseek-china-ai-chips.html&#34;&gt;by storm&lt;/a&gt;. Not only does it &lt;a href=&#34;https://arxiv.org/abs/2501.12948&#34;&gt;challenge&lt;/a&gt; the dominance of the currently best-performing reasoning model, o1 by OpenAI, but it achieves this &lt;a href=&#34;https://www.technologyreview.com/2025/01/24/1110526/china-deepseek-top-ai-despite-sanctions/&#34;&gt;despite&lt;/a&gt; US sanctions on cutting-edge AI chips. What&amp;rsquo;s more, the developers have open-sourced their research, allowing anyone to run their models locally.&lt;/p&gt;&#xA;&lt;p&gt;It&amp;rsquo;s worth first understanding what makes reasoning models special. In a nutshell, a reasoning LLM is designed to break down complex problems step-by-step, demonstrating its reasoning process rather than just generating an answer. A reasoning model mimics the human problem-solving process by explicitly walking through its decisions before producing a response. In contrast, popular models such as GPT-4o (used in ChatGPT) focus more on generating fluent, contextually appropriate responses by drawing from their trained knowledge, without necessarily revealing their internal reasoning path. While dominating the benchmarks, from a research perspective, the most interesting aspect of a state-of-the-art open-source reasoning model is the unrestricted access to its inner &amp;ldquo;thought process&amp;rdquo;.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenAI o3: Carbon Footprint</title>
      <link>http://localhost:1313/openai-o3-carbon-footprint/</link>
      <pubDate>Mon, 23 Dec 2024 13:12:20 +0100</pubDate>
      <guid>http://localhost:1313/openai-o3-carbon-footprint/</guid>
      <description>&lt;p&gt;OpenAI recently &lt;a href=&#34;https://openai.com/12-days/&#34;&gt;previewed&lt;/a&gt; &lt;strong&gt;o3&lt;/strong&gt;, the most powerful large language model up to date.&lt;/p&gt;&#xA;&lt;p&gt;While the performance capabilities (and their implications) are understandably getting most of the attention, what stood out to me was the cost of running the benchmarks. To evaluate the model, OpenAI partnered with the ARC Prize Foundation to assess its ability to efficiently acquire and apply new skills on the fly, rather than relying solely on recalling knowledge from its training data - a common limitation in today’s model benchmarking.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
