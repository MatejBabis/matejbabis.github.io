<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>home on Matej Babis</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in home on Matej Babis</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <copyright>Copyright © 2024, Matej Babis.</copyright>
    <lastBuildDate>Sat, 25 Jan 2025 11:30:52 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>DeepSeek-R1 &amp; Chinese Censorship</title>
      <link>http://localhost:1313/deepseek-r1-chinese-censorship/</link>
      <pubDate>Sat, 25 Jan 2025 11:30:52 +0100</pubDate>
      <guid>http://localhost:1313/deepseek-r1-chinese-censorship/</guid>
      <description>&lt;p&gt;DeepSeek-R1 is &lt;a href=&#34;https://www.forbes.com/sites/craigsmith/2025/01/22/deepseek-how-chinas-ai-innovators-are-challenging-the-status-quo/&#34;&gt;taking the world&lt;/a&gt; &lt;a href=&#34;https://www.nytimes.com/2025/01/23/technology/deepseek-china-ai-chips.html&#34;&gt;by the storm&lt;/a&gt;. Not only does it &lt;a href=&#34;https://arxiv.org/abs/2501.12948&#34;&gt;challenge&lt;/a&gt; the dominance of the currently best-performing reasoning model, o1 by OpenAI, but it achieved this &lt;a href=&#34;https://www.technologyreview.com/2025/01/24/1110526/china-deepseek-top-ai-despite-sanctions/&#34;&gt;despite&lt;/a&gt; US sanctions on cutting-edge AI chips. What&amp;rsquo;s more, the developers have open-sourced their research, allowing people to run the models locally.&lt;/p&gt;&#xA;&lt;p&gt;It&amp;rsquo;s worth understanding what makes reasoning models special. In a nutshell, a reasoning LLM is designed to break down complex problems step-by-step, demonstrating logical thinking and showing its work rather than just generating an answer. A reasoning model mimics the human problem-solving process by explicitly walking through its decisions before producing a response. In contrast, popular models such as GPT-4o (available via ChatGPT) focus more on generating fluent, contextually appropriate responses by drawing from their trained knowledge, without necessarily revealing their internal reasoning path. While dominating the benchmarks, from a research perspective, the most interesting aspect of a state-of-the-art open-source reasoning model is the unrestricted access to the its inner reasoning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>OpenAI o3: Carbon Footprint</title>
      <link>http://localhost:1313/openai-o3-carbon-footprint/</link>
      <pubDate>Mon, 23 Dec 2024 13:12:20 +0100</pubDate>
      <guid>http://localhost:1313/openai-o3-carbon-footprint/</guid>
      <description>&lt;p&gt;OpenAI recently &lt;a href=&#34;https://openai.com/12-days/&#34;&gt;previewed&lt;/a&gt; &lt;strong&gt;o3&lt;/strong&gt;, the most powerful large language model up to date.&lt;/p&gt;&#xA;&lt;p&gt;While the performance capabilities (and their implications) are understandably getting most of the attention, what stood out to me was the cost of running the benchmarks. To evaluate the model, OpenAI partnered with the ARC Prize Foundation to assess its ability to efficiently acquire and apply new skills on the fly, rather than relying solely on recalling knowledge from its training data - a common limitation in today’s model benchmarking.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
